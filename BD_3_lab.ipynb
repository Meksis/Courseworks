{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c78071e4",
   "metadata": {},
   "source": [
    "Команды для запуска hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add43108",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd\n",
    "hadoop-3.2.3/bin/hdfs namenode -format\n",
    "export PDSH_RCMD_TYPE=ssh\n",
    "start-all.sh\n",
    "hadoop fs -mkdir /user/\n",
    "hadoop fs -mkdir /user/abobus\n",
    "hadoop fs -put ~/Downloads/u.data /user/abobus\n",
    "hadoop fs -put ~/Downloads/u.item /user/abobus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00bad2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark as ps\n",
    "import json\n",
    "\n",
    "from pyspark import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "790ff2f4",
   "metadata": {},
   "source": [
    "Переменные, которые будем использовать далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1539292",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_path = 'hdfs://localhost:9000/user/abobus/u.data'\n",
    "movies_path = 'hdfs://localhost:9000/user/abobus/u.item'\n",
    "my_num = 212 + 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "234d204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 03:45:11 WARN Utils: Your hostname, BigData resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "22/12/12 03:45:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/12 03:45:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = ps.sql.SparkSession.builder.appName('xxx').getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4404df0d",
   "metadata": {},
   "source": [
    "Читаем файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7d28464",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ds = spark.read.csv(movies_path, sep = '|', encoding = 'cp1252', header = None)  # Читаем файл, задаем разделитель, устанавливаем кодировку, отбрасываем заголовки\n",
    "movies_ds = movies_ds.drop(*movies_ds.columns[2 : len(movies_ds.columns)])              # Убираем лишние столбцы, распаковывая список названий столбцов со второго до последнего\n",
    "movies_ds = movies_ds.withColumnRenamed('_c0', 'mID')                                   # Переименовываем столбец\n",
    "movies_ds = movies_ds.withColumnRenamed('_c1', 'Movie_name')                            # Переименовываем столбец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99129eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_ds = spark.read.csv(mark_path, sep = '\\t')                                        # Читаем файл, задаем разделитель\n",
    "\n",
    "marks_ds = marks_ds.withColumnRenamed('_c0', 'uID')                                     # Переименовываем столбец\n",
    "marks_ds = marks_ds.withColumnRenamed('_c1', 'mID')                                     # Переименовываем столбец\n",
    "marks_ds = marks_ds.withColumnRenamed('_c2', 'mark')                                    # Переименовываем столбец\n",
    "marks_ds = marks_ds.withColumnRenamed('_c3', 'timestamp')                               # Переименовываем столбец"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e947fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_ds.createOrReplaceTempView('Movies_db')                                          # Создаем представление таблицы с именем Movies_db\n",
    "marks_ds.createOrReplaceTempView('Marks_db')                                            # Создаем представление таблицы с именем Marks_db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb41952",
   "metadata": {},
   "source": [
    "В подзапросе выбираем id фильма, пользователя и оценку, которую поставил данный пользователь на этот фильм\n",
    "Было использовано объединение, учитывающее только пересекающиеся значения. Объединены таблица фильмов и оценок по айди фильма, где айди фильма - номер, выданный по заданию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae970f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|mark|quantity|\n",
      "+----+--------+\n",
      "|   1|       6|\n",
      "|   2|      36|\n",
      "|   3|      96|\n",
      "|   4|     171|\n",
      "|   5|      75|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'''\n",
    "with movie_mark (\n",
    "        select Movies_db.mID, uID, mark\n",
    "        from \n",
    "            Movies_db inner join Marks_db on Movies_db.mID = Marks_db.mID\n",
    "        where Movies_db.mID = {my_num}\n",
    "    )\n",
    "\n",
    "select mark, count(mark) as quantity\n",
    "from movie_mark\n",
    "group by mark\n",
    "order by mark\n",
    "''').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeda8669",
   "metadata": {},
   "source": [
    "В подзапросе выбираем айди фильма, пользователя и оценку фильма пользователем из объединенных таблиц фильмов и оцеок, где айли фильма совпадает с выданным по заданию номером.\n",
    "Выбираем оценки, количество оценок, группируем по оценке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00709d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_237 = spark.sql(f'''\n",
    "    with movie_mark (\n",
    "            select Movies_db.mID, uID, mark\n",
    "            from \n",
    "                Movies_db inner join Marks_db \n",
    "                    on Movies_db.mID = Marks_db.mID\n",
    "                where Movies_db.mID = {my_num}\n",
    "        )\n",
    "\n",
    "    select mark, count(mark) as quantity\n",
    "    from movie_mark\n",
    "        group by mark\n",
    "        order by mark\n",
    "''').collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34a0d362",
   "metadata": {},
   "source": [
    "В подзапросе выбираем название фильма, айди пользователя и его оценку фильму из объединения таблиц фильмов и оценок по совпадению айди фильма.\n",
    "Выбираем оценки, количество оценок из подзапроса, группируем по оценке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40ebe1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_all = spark.sql(f'''\n",
    "\n",
    "with movie_mark (\n",
    "        select Movies_db.Movie_name, uID, mark\n",
    "        from \n",
    "            Movies_db inner join Marks_db \n",
    "                on Movies_db.mID = Marks_db.mID\n",
    "    )\n",
    "    select mark, count(mark) as quantity\n",
    "    from movie_mark\n",
    "        group by mark\n",
    "        order by mark\n",
    "    \n",
    "''').collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9488d3f5",
   "metadata": {},
   "source": [
    "Переписываем результаты запросов в списки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73cc20d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks_237 = [res[1] for res in marks_237]\n",
    "marks_all = [res[1] for res in marks_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08dee9",
   "metadata": {},
   "source": [
    "Открываем json файл на запись, сохраняем в него словарь, где есть два ключа - marks_237 для фильма, чей айди был дан по заданию и marks_all для сбора сведений об оценках всех фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3d62ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('db_lab_3.json', 'w') as json_file:\n",
    "    json.dump(json.dumps({ 'rates_filmID_237' : marks_237, 'rates_films' : marks_all }), json_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9dae1f1a",
   "metadata": {},
   "source": [
    "Как выглядят записываемые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2f8a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"rates_filmID_237\": [6, 36, 96, 171, 75], \"rates_films\": [6110, 11370, 27145, 34174, 21201]}'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps({ 'rates_filmID_237' : marks_237, 'rates_films' : marks_all })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
